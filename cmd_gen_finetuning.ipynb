{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning an LLM for Command Generation in CALM\n",
    "\n",
    "This is a worked example of how to efficiently fine-tune a base language model from [Hugging Face Hub](https://huggingface.co/models) using the [TRL](https://huggingface.co/docs/trl/en/index) libraries for the task of command generation within [CALM](https://rasa.com/docs/rasa-pro/calm).\n",
    "\n",
    "To run fine-tuning, you must have first [generated the dataset](https://rasa.com/rasa-pro/docs/operating/fine-tuning-recipe) files `train.jsonl` and `val.jsonl`, which must be in the [TRL instruction format](https://huggingface.co/docs/trl/en/sft_trainer#dataset-format-support)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure fine-tuning environment\n",
    "\n",
    "In order to run this notebook, you will need to first install the necessary libraries onto a machine with the following minimum hardware requirements:\n",
    "- Single NVIDIA A100 GPU with 40GB VRAM\n",
    "- 12 core CPU with 85GB RAM\n",
    "- 250GB disk\n",
    "\n",
    "Here is an example of how to set up the environment:\n",
    "\n",
    "First, we provisioned a Linux instance with the appropriate hardware and the following software installed:\n",
    "- Python 3.10\n",
    "- CUDA Toolkit 12.1\n",
    "- PyTorch 2.2.\n",
    "\n",
    "Next, we installed the necessary packages as follows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%sh\n",
    "pip install \"torch==2.6.*\" \"accelerate==1.5.*\" \"peft==0.14.*\" \"bitsandbytes==0.45.*\" \"transformers==4.49.*\" \"trl==0.15.*\" \"vllm==0.8.2\" \"flash-attn==2.7.4.post1\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose base model\n",
    "\n",
    "You can download the model you want to fine-tune from Hugging Face Hub using the [official CLI](https://huggingface.co/docs/huggingface_hub/en/guides/cli) with an [API access token](https://huggingface.co/docs/transformers.js/en/guides/private#step-1-generating-a-user-access-token) as per the code below. Make sure you first update the `HUGGINGFACE_TOKEN` and `BASE_MODEL` environment variables with your own values.\n",
    "\n",
    "When testing this notebook, the [Llama 3.1 8B Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) model was used. Note that `meta-llama/Meta-Llama-3.1-8B-Instruct` is a [gated model](https://huggingface.co/docs/hub/en/models-gated) that you must first request access to before using.\n",
    "\n",
    "You can use any other PyTorch model available on [Hugging Face Hub](https://huggingface.co/models). It is recommended that you use a model that has been pre-trained on instructional tasks, such as the [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) model.\n",
    "\n",
    "Pre-trained models with more parameters will generally perform better at tasks than models with fewer parameters. However, the size of model you can use is limited by how much memory your GPU has.\n",
    "\n",
    "Alternatively, if you already have a PyTorch model directory to hand, you can upload it to your notebook environment manually."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TODO: update with your values\n",
    "%env BASE_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "%env HUGGINGFACE_TOKEN=CHANGEME\n",
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "\n",
    "# download model\n",
    "!huggingface-cli download \"$BASE_MODEL\" \\\n",
    "    --token \"$HUGGINGFACE_TOKEN\" \\\n",
    "    --local-dir \"./base_model\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and quantize base model\n",
    "\n",
    "The [quantization of model parameters](https://huggingface.co/docs/optimum/en/concept_guides/quantization) can significantly reduce the GPU memory required to run model fine-tuning and inference, at the cost of model accuracy.\n",
    "\n",
    "Here, the base model is loaded from disk and can be quantized into an 4-bit representation on the fly using the [BitsAndBytes](https://huggingface.co/docs/transformers/main/en/quantization/bitsandbytes) library."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import torch\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "do_quantization = False # adjust this setting based on whether you want to use quantization or not\n",
    "BASE_MODEL_PATH = \"./base_model\"\n",
    "\n",
    "def get_model_and_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "\n",
    "    bnb_config = None\n",
    "\n",
    "    if do_quantization:\n",
    "        # 4-bit quantization configuration\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    if do_quantization:\n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model,\n",
    "            use_gradient_checkpointing=True,\n",
    "        )\n",
    "    model.config.use_cache = False\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer(BASE_MODEL_PATH)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure base model for PEFT\n",
    "\n",
    "[Parameter Efficient Fine-Tuning](https://huggingface.co/blog/peft) (PEFT) is a technique for adapting LLMs for specific tasks by freezing all of the base model parameters and only training a relatively small number of additional parameters. Compared to fine-tuning all parameters, PEFT can significantly reduce the amount of GPU memory required at the cost of the fine-tuned model accuracy.\n",
    "\n",
    "In the code below, the base model is configured for PEFT using the [Low-Rank Adaptation](https://arxiv.org/pdf/2106.09685) (LoRA) method. It is recommended that you read the [official documentation](https://github.com/huggingface/peft) and experiment with the parameters of the `LoraConfig` class. For example, you may get better model performance with different values for `r` and `lora_alpha`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\n",
    "    [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load training and validation datasets\n",
    "\n",
    "The following code loads the training and validation datasets from the `train.jsonl` and `val.jsonl` files, respectively\n",
    "\n",
    "As the files use the TRL instruction format, the TRL trainer used later will be able to [automatically parse](https://huggingface.co/docs/trl/en/sft_trainer#dataset-format-support) the datasets and [generate the prompts from a template](https://huggingface.co/docs/transformers/en/chat_templating) configured in the tokenizer.\n",
    "\n",
    "Prompt templates vary between models and TRL will infer the correct template from your base model. If this is not available in your base model or if you wish to change it, you can set your own [template string](https://huggingface.co/docs/transformers/en/chat_templating#advanced-adding-and-editing-chat-templates) manually."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import datasets\n",
    "\n",
    "# Load the training and evaluation datasets from JSONL files on disk\n",
    "train_dataset = datasets.load_dataset(\n",
    "    \"json\", data_files={\"train\": \"train.jsonl\"}, split=\"train\"\n",
    ")\n",
    "eval_dataset = datasets.load_dataset(\n",
    "    \"json\", data_files={\"eval\": \"val.jsonl\"}, split=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "# Uncomment the following line if you want to test prompt formatting on a single example from the eval dataset\n",
    "# print(get_formatting_func_from_dataset(train_dataset, tokenizer)(eval_dataset[0]))\n",
    "\n",
    "# Define a function to format prompts for each example in the dataset\n",
    "def formatting_prompts_func(examples):\n",
    "    # Extract conversation messages from each example\n",
    "    convos = examples[\"messages\"]\n",
    "\n",
    "    # Apply the chat template to each conversation without tokenizing or adding generation prompts\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "\n",
    "    # Return the formatted texts in a new dictionary key\n",
    "    return {\"text\": texts}\n",
    "\n",
    "\n",
    "# Apply the formatting function to both the training and evaluation datasets in batches\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure trainer\n",
    "\n",
    "Below, the arguments for the supervised fine-tuning (SFT) trainer are configured. Their values were chosen somewhat arbitrarily and resulted in satisfactory results during testing.\n",
    "\n",
    "It is recommended that you read the official documentation and experiment with the arguments passed to `SFTConfig` (see [here](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTTrainer)) and `SFTTrainer` (see [here](https://huggingface.co/docs/trl/main/en/sft_trainer#trl.SFTTrainer)).\n",
    "\n",
    "For example:\n",
    "- If you get an OOM error when running fine-tuning, you can reduce `per_device_train_batch_size` in order to reduce the memory footprint. However, if your GPU has sufficient memory, you can try increasing it in order to reduce the total number of training steps.\n",
    "- Consider setting `max_steps`, as you may not need to perform all epochs in order to achieve optimal model accuracy. Conversely, you may see better model accuracy by increasing `num_train_epochs`.\n",
    "- If fine-tuning is taking too long, you can increase `eval_steps` in order to reduce how often validation is performed. \n",
    "\n",
    "Response template used in the `DataCollatorForCompletionOnlyLM` currently can't be loaded automatically for each model, so this string needs to be changed based on the model. The example defined here is used in LLaMA-3 models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "\n",
    "max_seq_length = 4096\n",
    "\n",
    "# configure training args\n",
    "args = SFTConfig(\n",
    "    ###### training\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=20,\n",
    "    # max_steps = 1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    ###### datatypes\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    ###### evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    per_device_eval_batch_size=2,\n",
    "    ###### outputs\n",
    "    logging_steps=20,\n",
    "    output_dir=\"outputs\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=False\n",
    ")\n",
    "\n",
    "response_template = \"assistant<|end_header_id|>\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "# setup trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    peft_config=peft_config\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Perform supervised fine-tuning\n",
    "\n",
    "In the code below, fine-tuning is performed using the previously congfigured trainer.\n",
    "\n",
    "When testing this step on an NVIDIA A100 using the configuration defined above, it took around 1 hour to perform fine-tuning with a training dataset containing around 1600 examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "source": [
    "# run fine-tuning\n",
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Persisting the trained model\n",
    "\n",
    "After fine-tuning, you can save the LoRA adapter weights, allowing you to later load them on top of the base model for inference."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pathlib\n",
    "\n",
    "FINETUNED_MODEL_PATH = pathlib.Path(\"./finetuned_model\")\n",
    "\n",
    "FINETUNED_MODEL_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# `model` is your PEFT-wrapped model, `tokenizer` the tokenizer you trained with\n",
    "model.save_pretrained(FINETUNED_MODEL_PATH)\n",
    "tokenizer.save_pretrained(FINETUNED_MODEL_PATH)\n",
    "\n",
    "print(\"âœ“ LoRA adapter written to\", FINETUNED_MODEL_PATH.resolve())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize fine-tuning metrics\n",
    "\n",
    "Some of the metrics collected during fine-tuning are visualised below in order for you to diagnose any potential issues with the model.\n",
    "\n",
    "Specifically, the training and validation losses are plotted against the training step number. Please check the plot for the following:\n",
    "- Ideally, as the fine-tuning steps increase, the training and validation losses should decrease and converge. \n",
    "- If both loss curves do not converge, it may be worth performing more fine-tuning steps or epochs. This is known as [underfitting](https://www.ibm.com/topics/underfitting).\n",
    "- If the validation loss suddenly starts to increase while the training loss continues to decrease or converge, you should decrease your total number of steps or epochs. This is known as [overfitting](https://www.ibm.com/topics/overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install pandas matplotlib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot step against train and val losses\n",
    "fig, ax = plt.subplots()\n",
    "log_history = pd.DataFrame(trainer.state.log_history)\n",
    "eval_loss = log_history[[\"step\", \"eval_loss\"]].dropna().plot(x=\"step\", ax=ax)\n",
    "train_loss = log_history[[\"step\", \"train_loss\"]].dropna().plot(x=\"step\", ax=ax)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run ad hoc inference\n",
    "\n",
    "You can load your fine-tuned model from disk using HuggingFace transformers and use it to run optimized inference on individual inputs of your choosing using the code below.\n",
    "\n",
    "Note that the inputs passed to model are in the [TRL convertsational format](https://huggingface.co/docs/trl/en/sft_trainer#dataset-format-support) as the Hugging Face [chat template requires them to be](https://huggingface.co/docs/transformers/main/en/chat_templating#how-do-i-use-chat-templates). During training TRL will [automatically convert the instruction format to the conversational format](https://github.com/huggingface/trl/blob/main/trl/extras/dataset_formatting.py). However, you have to do this yourself when applying chat templates manually for inference."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import torch, pathlib, os\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TextStreamer,\n",
    ")\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "ADAPTER_DIR   = pathlib.Path(\"./finetuned_model\")\n",
    "dtype         = torch.bfloat16\n",
    "\n",
    "bnb_cfg = None\n",
    "if do_quantization:\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "peft_cfg   = PeftConfig.from_pretrained(ADAPTER_DIR)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=dtype,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "\n",
    "# (optional) merge for slightly faster inference\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "\n",
    "model.eval()\n",
    "model.config.use_cache = True\n",
    "\n",
    "content = eval_dataset[0][\"messages\"]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": content}],  # in the TRL conversational format\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": content}],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer)\n",
    "with torch.inference_mode():\n",
    "    _ = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,\n",
    "        streamer=streamer,\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 11. Serve with vLLM\n",
    "\n",
    "You can also deploy the model via vLLM library using the command below. If you are not doing quantization, remove `--quantization bitsandbytes` parameter.\n",
    "For any further adjustments and parameterization, check out the [official vLLM documentation](https://docs.vllm.ai/en/latest/)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%env HF_TOKEN=CHANGEME  # Same as the `HUGGINGFACE_TOKEN`, but needs to be set like this for vLLM to work\n",
    "\n",
    "%%sh\n",
    "vllm serve meta-llama/Llama-3.1-8B-Instruct \\\n",
    "--quantization bitsandbytes \\\n",
    "--dtype bfloat16 \\\n",
    "--enable-lora \\\n",
    "--lora-modules custom_lora=finetuned_model \\\n",
    "--swap-space 16 \\\n",
    "--max-model-len 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export fine-tuned model\n",
    "\n",
    "Lastly, export your fine-tuned model directory to an appropriate storage location that can be easily accessed later for [deployment](https://rasa.com/rasa-pro/docs/building-assistants/self-hosted-llm).\n",
    "\n",
    "It is recommended that you use a cloud object store, such as [Amazon S3](https://aws.amazon.com/s3/) or [Google Cloud Storage](https://cloud.google.com/storage).\n",
    "\n",
    "Uncomment and run the corresponding commands below for your cloud provider, making sure to first update the environment variables with your own values. It is assumed that:\n",
    "- your bucket already exists\n",
    "- you have already installed the CLI tool for your cloud provider\n",
    "- you have already authenticated with your cloud provider and have sufficient permissions to write to your bucket"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%sh\n",
    "export LOCAL_MODEL_PATH=\"./finetuned_model\"\n",
    "\n",
    "# if using amazon\n",
    "# export S3_MODEL_URI=\"s3://CHANGEME\" # update with your value\n",
    "# aws s3 cp \"${LOCAL_MODEL_PATH}\" \"${S3_MODEL_URI}\" --recursive\n",
    "\n",
    "# if using google\n",
    "# export GCS_MODEL_URI=\"gs://CHANGEME\" # update with your value\n",
    "# gsutil cp -r \"${LOCAL_MODEL_PATH}\" \"${GCS_MODEL_URI}\""
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
